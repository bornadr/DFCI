{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94e091b-a437-4eea-b7ec-8ebba184e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94319bdc-d92a-4a73-b7bc-70e2e77254d3",
   "metadata": {},
   "source": [
    "## TASK 1 - FASTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb00f9d6-aeba-467a-9fb1-be91d64abf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fastq_files(root=\".\"): \n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(('.fastq','.fq')):\n",
    "                yield os.path.join(dirpath, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f5dc61-f21e-4b6e-a763-db411ee5a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## my logic for this sub task is a function that recieves the file path as input, we traverse through the file\n",
    "## line by line until we reach end of file(the header returns a empty string). We also keep track of the sequence \n",
    "## which we then compare to the minimum length, if it's higher then we add one to our counter and in either case\n",
    "## we add 1 to our total counter which keeps track of total \"reads\" or \"DNA fragments\" which we then use to calculate \n",
    "## the percentage. This approach is recursive and also doesn't require large- memory overhead. \n",
    "\n",
    "def calc_pct_long_reads(fastq_path, min_len=30):\n",
    "    total = 0\n",
    "    over = 0\n",
    "    with open(fastq_path, 'r') as f:\n",
    "        while True:\n",
    "            header = f.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            seq = f.readline().strip()\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "            total += 1\n",
    "            if len(seq) > min_len:\n",
    "                over += 1\n",
    "    if total > 0:\n",
    "        pct = (over/total) * 100\n",
    "    else:\n",
    "        pct = 0\n",
    "    return total, over, pct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fbd2d7e-7a85-46a0-b378-6b57b28705e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sample_files/fastq/read2/Sample_R2.fastq: 83.60% (989/1183) reads > 30 nt\n",
      "./sample_files/fastq/read1/Sample_R1.fastq: 80.64% (954/1183) reads > 30 nt\n",
      "./sample_files/fastq/read1/.ipynb_checkpoints/Sample_R1-checkpoint.fastq: 80.64% (954/1183) reads > 30 nt\n"
     ]
    }
   ],
   "source": [
    "## runnning the function in current directory\n",
    "for fq in find_fastq_files('.'):\n",
    "    total, over, pct = calc_pct_long_reads(fq, 30)\n",
    "    print(f\"{fq}: {pct:.2f}% ({over}/{total}) reads > 30 nt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7d78d-2206-4deb-a374-b67c50951be4",
   "metadata": {},
   "source": [
    "## TASK 1 - FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b4f59d-c6ce-4219-a0d4-2856812062cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I use a dictionary to keep count of the sequences, the logic is pretty simple we read each line \n",
    "## and identify if it's the start of a sequence or not if it is we look for it in the dictionary and add one to it\n",
    "## counter, returning the final dictionary. We then go through the items in dict and sort the top 10 most frequent ones\n",
    "def parse_fasta(path):\n",
    "    counts = {}\n",
    "    seq = ''\n",
    "\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if seq:\n",
    "                    counts[seq] = counts.get(seq,0) + 1\n",
    "                seq = ''\n",
    "            else:\n",
    "                seq += line\n",
    "        if seq:\n",
    "            counts[seq] = counts.get(seq,0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31e55afc-3c85-46df-89ee-67bdfde323b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count\tSequence\n",
      "28\tCGCGCAGGCTGAAGTAGTTACGCCCCTGTAAAGGAATCTATGGACAATGGAACGAACA\n",
      "27\tTGTTCTGAGTCAAATGATATTAACTATGCTTATCACATATTATAAAAGACCGTGGACATTCATCTTTAGTGTGTCTCCCTCTTCCTACT\n",
      "22\tCTCAATCTGCCAAGACCATAGATCCTCTCTTACTGTCAGCTCATCCGGTGAGGCC\n",
      "21\tCCTGTTGCTGACTCAAGACATTAGTGAGAAATAAGACTTCTGCGATGCTCACCACTGCAATTGCTCATGCAAAATTGCGTTTAACAGG\n",
      "17\tATTGCGAATTCCGCCTGTGTCCCCCACACGAGCGTGAATCGTGGCTAGAAGTTCAGCCCCTCTTAGCACAGAGTGAG\n",
      "17\tTTTCAGCTGTCTTTTAAGCAGAAGCGATTTGTCCAACAAAAACAACGCTGTTTACGAA\n",
      "8\tGACACAAACACCGTGGCTCAACCTAATCCTATTAGAGCCGAAAAGGCGAGGATGCTGATTGAGTAGGTATCTGGA\n",
      "8\tTCACGCAGACAACGAACTGTGTCTGGATCAAAGACATCCGATAAGGCGATTCGTCTAGAAGGGTTACACAGTTGGGACCGGTAG\n",
      "5\tTGCTTAAACTCATGATAGTCCCTGAGTAAACTGGTTGCGACACGGCTCCCG\n",
      "5\tTGTGCAGAATATAATGTAAAAAAAACAGGACCCGGCTCTGTGCCGTTGGCCTGCGCGGTACTCATGTTAGTTTTCCGACTCCGACTTAT\n"
     ]
    }
   ],
   "source": [
    "fasta_path = \"sample_files/fasta/sample.fasta\"\n",
    "counts_dict = parse_fasta(fasta_path)\n",
    "\n",
    "top10 = sorted(\n",
    "    counts_dict.items(), \n",
    "    key=lambda pair: pair[1], \n",
    "    reverse=True\n",
    ")[:10]\n",
    "## results\n",
    "print(\"Count\\tSequence\")\n",
    "for seq, cnt in top10:\n",
    "    print(f\"{cnt}\\t{seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38acfeed-f1cc-43bf-b6f8-ed373049f38a",
   "metadata": {},
   "source": [
    "## Task 1 - Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2898c3c1-dfa7-4e95-8165-0dc348fd48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## my logic for this sub task is to first parse the GTF annotation file once,\n",
    "## extracting only the “gene” features and pulling out chromosome, start, end, and gene_name.\n",
    "## I store these as tuples in a dict then sort each chromosome’s list \n",
    "## by start coordinate and also keep a parallel list of just the start positions.\n",
    "## This gives me a O(logn) lookup structure\n",
    "## Next, I read the coordinates file line by line For each entry,\n",
    "## I check if the chromosome exists in my index, if so, I use bisect_right on the starts list\n",
    "## to get an insertion point i. \n",
    "## Finally, I write out a new TSV where each input line (chrom, pos) is appended with the\n",
    "## gene_name. This approach loads the GTF into memory only once,\n",
    "## uses pure built‑ins, and handles millions of lookups efficiently with minimal overhead.\n",
    "import re\n",
    "import bisect\n",
    "\n",
    "def build_gene_index(gtf_path):\n",
    "    \"\"\"\n",
    "    Parse only the “gene” entries from the GTF and build:\n",
    "      • index: { chrom: [(start,end,gene_name), …] }\n",
    "      • starts: { chrom: [start1, start2, …] }  # for bisect\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "    attr_pattern = re.compile(r'(\\S+)\\s+\"([^\"]+)\"')\n",
    "    with open(gtf_path) as gtf:\n",
    "        for line in gtf:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            cols = line.rstrip('\\n').split('\\t')\n",
    "            chrom, feature, start, end, attrs = cols[0], cols[2], int(cols[3]), int(cols[4]), cols[8]\n",
    "            if feature != 'gene':\n",
    "                continue\n",
    "            # extract gene_name (fallback to gene_id if missing)\n",
    "            attr_dict = {k:v for k,v in attr_pattern.findall(attrs)}\n",
    "            gene = attr_dict.get('gene_name', attr_dict.get('gene_id', 'NA'))\n",
    "            index.setdefault(chrom, []).append((start, end, gene))\n",
    "\n",
    "    ## sort each chrom’s list and build a parallel list of starts\n",
    "    starts = {}\n",
    "    for chrom, ivals in index.items():\n",
    "        ivals.sort(key=lambda x: x[0])\n",
    "        starts[chrom] = [iv[0] for iv in ivals]\n",
    "        index[chrom] = ivals\n",
    "\n",
    "    return index, starts\n",
    "\n",
    "# build it once\n",
    "gtf_path = 'sample_files/gtf/hg19_annotations.gtf'\n",
    "gene_index, gene_starts = build_gene_index(gtf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe364025-ee5c-4db9-9204-789ee3cd535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written annotated results to coordinates_annotated.tsv\n"
     ]
    }
   ],
   "source": [
    "def annotate_positions(coord_path, index, starts, out_path):\n",
    "    \"\"\"\n",
    "    For each line “chrom<TAB>pos” in coord_path, \n",
    "    find which gene (if any) contains pos, and write:\n",
    "      chrom<TAB>pos<TAB>gene_name/intergenic\n",
    "    \"\"\"\n",
    "    with open(coord_path) as fin, open(out_path, 'w') as fout:\n",
    "        for line in fin:\n",
    "            chrom, pos = line.split('\\t')\n",
    "            pos = int(pos)\n",
    "            gene_name = 'intergenic'\n",
    "            if chrom in index:\n",
    "                st_list = starts[chrom]\n",
    "                ivals   = index[chrom]\n",
    "                i = bisect.bisect_right(st_list, pos)\n",
    "                if i:\n",
    "                    s,e,name = ivals[i-1]\n",
    "                    if e >= pos:\n",
    "                        gene_name = name\n",
    "            fout.write(f\"{chrom}\\t{pos}\\t{gene_name}\\n\")\n",
    "\n",
    "## running it\n",
    "coords = 'sample_files/annotate/coordinates_to_annotate.txt'\n",
    "out_file = 'coordinates_annotated.tsv'\n",
    "annotate_positions(coords, gene_index, gene_starts, out_file)\n",
    "print(f\"Written annotated results to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401afb4f-2e3c-4da3-be86-db061fc84845",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d9d523d-c256-42a6-b3ff-130450c53327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC_bin\tmean_target_coverage\n",
      "0-10%\t0.00\n",
      "10-20%\t69.29\n",
      "20-30%\t77.93\n",
      "30-40%\t99.02\n",
      "40-50%\t101.28\n",
      "50-60%\t92.12\n",
      "60-70%\t78.93\n",
      "70-80%\t37.83\n",
      "80-90%\t10.28\n"
     ]
    }
   ],
   "source": [
    "## my logic for this sub task is to define 10 equal GC‐content bins based on the %gc value\n",
    "## I initialize a dictionary to accumulate total coverage and count per bin.\n",
    "## Then, I open the file and read it line by line with a CSV DictReader, stripping out any intervals with missing coverage.\n",
    "## For each valid row, I convert the GC fraction and mean coverage to floats, compute which bin index it falls into\n",
    "## (multiplying GC by 10 and capping at 9 for gc==1.0), and then add the coverage to that bin’s running sum and increment its counter.\n",
    "## After processing every interval, I loop over the bins in order, compute the average coverage (sum/count) for each one,\n",
    "## and print out the bin label alongside its mean target coverage. This approach uses only built‐ins, runs in one pass,\n",
    "## and keeps memory usage small since I never load the entire file at once.```\n",
    "\n",
    "import csv\n",
    "bin_edges = [i/10 for i in range(11)]\n",
    "labels = [f\"{int(bin_edges[i]*100)}-{int(bin_edges[i+1]*100)}%\" for i in range(len(bin_edges)-1)]\n",
    "\n",
    "stats = {label: [0.0, 0] for label in labels}\n",
    "\n",
    "with open('Example.hs_intervals.txt') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        # pull and strip both fields\n",
    "        gc_str  = row['%gc'].strip()\n",
    "        cov_str = row['mean_coverage'].strip()\n",
    "\n",
    "        # if either is missing, skip this interval\n",
    "        if not gc_str or not cov_str:\n",
    "            continue\n",
    "\n",
    "        gc  = float(gc_str)\n",
    "        cov = float(cov_str)\n",
    "\n",
    "        # assign to bin as before...\n",
    "        idx = min(int(gc*10), 9)\n",
    "        label = labels[idx]\n",
    "        stats[label][0] += cov\n",
    "        stats[label][1] += 1\n",
    "\n",
    "\n",
    "## computing and print means\n",
    "print(\"GC_bin\\tmean_target_coverage\")\n",
    "for label in labels:\n",
    "    total_cov, count = stats[label]\n",
    "    if count > 0:\n",
    "        mean_cov = total_cov / count\n",
    "        print(f\"{label}\\t{mean_cov:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abcaf6-6656-45a1-abf1-82bcc6b1314d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
